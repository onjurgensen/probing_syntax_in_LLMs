{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa2c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import transformer_lens\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from io import StringIO, BytesIO\n",
    "from conllu import parse_incr\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image as PILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d93e9bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(nn.Module):\n",
    "  pass\n",
    "\n",
    "class TwoWordPSDProbe(Probe):\n",
    "  \"\"\" Computes squared L2 distance after projection by a matrix.\n",
    "\n",
    "  For a batch of sentences, computes all n^2 pairs of distances\n",
    "  for each sentence in the batch.\n",
    "  \"\"\"\n",
    "  def __init__(self, args):\n",
    "    print('Constructing TwoWordPSDProbe')\n",
    "    super(TwoWordPSDProbe, self).__init__()\n",
    "    self.args = args\n",
    "    self.probe_rank = args['probe']['maximum_rank']\n",
    "    self.model_dim = args['model']['hidden_dim']\n",
    "\n",
    "    # Initialize projection matrix\n",
    "    self.proj = nn.Parameter(data = torch.zeros(self.model_dim, self.probe_rank))\n",
    "    nn.init.uniform_(self.proj, -0.05, 0.05)\n",
    "    self.to(args['device'])\n",
    "\n",
    "  def forward(self, batch):\n",
    "    \"\"\" Computes all n^2 pairs of distances after projection\n",
    "    for each sentence in a batch.\n",
    "\n",
    "    Note that due to padding, some distances will be non-zero for pads.\n",
    "    Computes (B(h_i-h_j))^T(B(h_i-h_j)) for all i,j\n",
    "\n",
    "    Args:\n",
    "      batch: a batch of word representations of the shape\n",
    "        (batch_size, max_seq_len, representation_dim)\n",
    "    Returns:\n",
    "      A tensor of distances of shape (batch_size, max_seq_len, max_seq_len)\n",
    "    \"\"\"\n",
    "    batch = batch.to(self.proj.device) # added\n",
    "\n",
    "    transformed = torch.matmul(batch, self.proj)\n",
    "    batchlen, seqlen, rank = transformed.size()\n",
    "    transformed = transformed.unsqueeze(2)\n",
    "    transformed = transformed.expand(-1, -1, seqlen, -1)\n",
    "    transposed = transformed.transpose(1,2).to(self.proj.device)\n",
    "    diffs = transformed - transposed\n",
    "    squared_diffs = diffs.pow(2).to(self.proj.device)\n",
    "    squared_distances = torch.sum(squared_diffs, -1)\n",
    "    return squared_distances\n",
    "\n",
    "def get_data():\n",
    "    sentences_train = []\n",
    "    sentences_test = []\n",
    "\n",
    "    root_url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/\"\n",
    "    \n",
    "    files = {\n",
    "        \"test\": \"en_ewt-ud-test.conllu\", \n",
    "        \"train\": \"en_ewt-ud-train.conllu\"\n",
    "        }\n",
    "\n",
    "    for key, file in files.items():\n",
    "        file_url = os.path.join(root_url, file)\n",
    "        response = requests.get(file_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "\n",
    "        file_content = StringIO(response.text)\n",
    "        for sentence in parse_incr(file_content):\n",
    "            if key == \"test\":\n",
    "                sentences_test.append(sentence)\n",
    "            if key == \"train\":\n",
    "                sentences_train.append(sentence)\n",
    "\n",
    "    # exlude email addresses and URLs\n",
    "    email_pattern = re.compile(r'\\S+@\\S+')\n",
    "    url_pattern = re.compile(r'http\\S+|www\\S+')\n",
    "\n",
    "    sentences_train = [s for s in sentences_train if not email_pattern.search(s.metadata[\"text\"]) and not url_pattern.search(s.metadata[\"text\"])]\n",
    "    sentences_test =  [s for s in sentences_test  if not email_pattern.search(s.metadata[\"text\"]) and not url_pattern.search(s.metadata[\"text\"])]\n",
    "    \n",
    "    return sentences_train, sentences_test\n",
    "\n",
    "class SyntaxProbeDataset(Dataset):\n",
    "    def __init__(self, sentences, model_name, layer_name):\n",
    "        self.sentences = []\n",
    "        self.activations = []\n",
    "        self.matrices = []\n",
    "        model = transformer_lens.HookedTransformer.from_pretrained(model_name)\n",
    "        for sentence in tqdm(sentences, desc=\"Processing sentences\"):\n",
    "            text = sentence.metadata[\"text\"]\n",
    "            activations = combine_token_embeddings(text, [tok[\"form\"] for tok in sentence], model, layer_name)\n",
    "            activation = activations.cpu().numpy()\n",
    "            mat, _, _, _ = distance_matrix_from_sentence(sentence)\n",
    "                \n",
    "            if activation.shape[0] > 0 and mat.shape[0] > 0:\n",
    "                self.sentences.append(sentence)\n",
    "                self.activations.append(activation)\n",
    "                self.matrices.append(mat)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.activations[idx], self.matrices[idx]\n",
    "    \n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Function to collate a batch of (activation, matrix) tuples by padding them to the same length.\"\"\"\n",
    "    # batch: list of (activation, matrix) tuples\n",
    "    activations, matrices = zip(*batch)\n",
    "    batch_size = len(activations)\n",
    "    seq_lens = [a.shape[0] for a in activations]\n",
    "    max_len = max(seq_lens)\n",
    "    hidden_dim = activations[0].shape[1]\n",
    "\n",
    "    # Pad activations\n",
    "    padded_activations = np.zeros((batch_size, max_len, hidden_dim), dtype=np.float32)\n",
    "    for i, act in enumerate(activations):\n",
    "        padded_activations[i, :act.shape[0], :] = act\n",
    "\n",
    "    # Pad matrices\n",
    "    padded_matrices = np.full((batch_size, max_len, max_len), fill_value=-1, dtype=np.float32)\n",
    "    for i, mat in enumerate(matrices):\n",
    "        l = mat.shape[0]\n",
    "        padded_matrices[i, :l, :l] = mat\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    padded_activations = torch.tensor(padded_activations)\n",
    "    padded_matrices = torch.tensor(padded_matrices)\n",
    "    lengths = torch.tensor(seq_lens)\n",
    "\n",
    "    return padded_activations, padded_matrices, lengths, None\n",
    "\n",
    "class ProbeRegimen:\n",
    "  \"\"\"Basic regimen for training and running inference on probes.\n",
    "  \n",
    "  Tutorial help from:\n",
    "  https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "  Attributes:\n",
    "    optimizer: the optimizer used to train the probe\n",
    "    scheduler: the scheduler used to set the optimizer base learning rate\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, args):\n",
    "    self.args = args\n",
    "    self.max_epochs = args['probe_training']['epochs']\n",
    "    self.params_path = os.path.join(args['reporting']['root'], args['probe']['params_path'])\n",
    "\n",
    "  def set_optimizer(self, probe):\n",
    "    \"\"\"Sets the optimizer and scheduler for the training regimen.\n",
    "  \n",
    "    Args:\n",
    "      probe: the probe PyTorch model the optimizer should act on.\n",
    "    \"\"\"\n",
    "    self.optimizer = optim.Adam(probe.parameters(), lr=0.001)\n",
    "    self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1,patience=0)\n",
    "\n",
    "  def train_until_convergence(self, probe, model, loss, train_dataset, dev_dataset):\n",
    "    \"\"\" Trains a probe until a convergence criterion is met.\n",
    "\n",
    "    Trains until loss on the development set does not improve by more than epsilon\n",
    "    for 5 straight epochs.\n",
    "\n",
    "    Writes parameters of the probe to disk, at the location specified by config.\n",
    "\n",
    "    Args:\n",
    "      probe: An instance of probe.Probe, transforming model outputs to predictions\n",
    "      model: An instance of model.Model, transforming inputs to word reprs\n",
    "      loss: An instance of loss.Loss, computing loss between predictions and labels\n",
    "      train_dataset: a torch.DataLoader object for iterating through training data\n",
    "      dev_dataset: a torch.DataLoader object for iterating through dev data\n",
    "    \"\"\"\n",
    "    self.set_optimizer(probe)\n",
    "    min_dev_loss = sys.maxsize\n",
    "    min_dev_loss_epoch = -1\n",
    "\n",
    "    self.train_losses = []\n",
    "    self.val_losses = []\n",
    "    \n",
    "    for epoch_index in tqdm(range(self.max_epochs), desc='[training]'):\n",
    "      epoch_train_loss = 0\n",
    "      epoch_dev_loss = 0\n",
    "      epoch_train_epoch_count = 0\n",
    "      epoch_dev_epoch_count = 0\n",
    "      epoch_train_loss_count = 0\n",
    "      epoch_dev_loss_count = 0\n",
    "      for batch in tqdm(train_dataset, desc='[training batch]'):\n",
    "        probe.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        observation_batch, label_batch, length_batch, _ = batch\n",
    "        word_representations = model(observation_batch)\n",
    "        predictions = probe(word_representations)\n",
    "        batch_loss, count = loss(predictions, label_batch, length_batch)\n",
    "        batch_loss.backward()\n",
    "        epoch_train_loss += batch_loss.detach().cpu().numpy()*count.detach().cpu().numpy()\n",
    "        epoch_train_epoch_count += 1\n",
    "        epoch_train_loss_count += count.detach().cpu().numpy()\n",
    "        self.optimizer.step()\n",
    "      for batch in tqdm(dev_dataset, desc='[dev batch]'):\n",
    "        self.optimizer.zero_grad()\n",
    "        probe.eval()\n",
    "        observation_batch, label_batch, length_batch, _ = batch\n",
    "        word_representations = model(observation_batch)\n",
    "        predictions = probe(word_representations)\n",
    "        batch_loss, count = loss(predictions, label_batch, length_batch)\n",
    "        epoch_dev_loss += batch_loss.detach().cpu().numpy()*count.detach().cpu().numpy()\n",
    "        epoch_dev_loss_count += count.detach().cpu().numpy()\n",
    "        epoch_dev_epoch_count += 1\n",
    "\n",
    "\n",
    "      # Compute average losses and store them\n",
    "      avg_train_loss = epoch_train_loss / epoch_train_loss_count\n",
    "      avg_dev_loss = epoch_dev_loss / epoch_dev_loss_count\n",
    "      self.train_losses.append(avg_train_loss)\n",
    "      self.val_losses.append(avg_dev_loss)\n",
    "        \n",
    "      self.scheduler.step(epoch_dev_loss)\n",
    "      tqdm.write('[epoch {}] Train loss: {}, Dev loss: {}'.format(epoch_index, epoch_train_loss/epoch_train_loss_count, epoch_dev_loss/epoch_dev_loss_count))\n",
    "      if epoch_dev_loss / epoch_dev_loss_count < min_dev_loss - 0.0001:\n",
    "        torch.save(probe.state_dict(), self.params_path)\n",
    "        min_dev_loss = epoch_dev_loss / epoch_dev_loss_count\n",
    "        min_dev_loss_epoch = epoch_index\n",
    "        tqdm.write('Saving probe parameters')\n",
    "      elif min_dev_loss_epoch < epoch_index - 4:\n",
    "        tqdm.write('Early stopping')\n",
    "        break\n",
    "\n",
    "  def predict(self, probe, model, dataset):\n",
    "    \"\"\" Runs probe to compute predictions on a dataset.\n",
    "\n",
    "    Args:\n",
    "      probe: An instance of probe.Probe, transforming model outputs to predictions\n",
    "      model: An instance of model.Model, transforming inputs to word reprs\n",
    "      dataset: A pytorch.DataLoader object \n",
    "\n",
    "    Returns:\n",
    "      A list of predictions for each batch in the batches yielded by the dataset\n",
    "    \"\"\"\n",
    "    probe.eval()\n",
    "    predictions_by_batch = []\n",
    "    for batch in tqdm(dataset, desc='[predicting]'):\n",
    "      observation_batch, label_batch, length_batch, _ = batch\n",
    "      word_representations = model(observation_batch)\n",
    "      predictions = probe(word_representations)\n",
    "      predictions_by_batch.append(predictions.detach().cpu().numpy())\n",
    "    return predictions_by_batch\n",
    "\n",
    "def get_matrices(sentences):\n",
    "    \"\"\"function to get distance matrices for a list of sentences\"\"\"\n",
    "    matrices = []\n",
    "    for sent in sentences:\n",
    "        mat, _, _, _ = distance_matrix_from_sentence(sent)\n",
    "        matrices.append(mat)\n",
    "    return matrices\n",
    "\n",
    "def combine_token_embeddings(text, word_list, model, layer_name):\n",
    "    \"\"\"\n",
    "    Given a sentence string, a list of words (from UD), and a model,\n",
    "    returns a tensor of shape (num_words, hidden_dim) where each row\n",
    "    is the combined embedding for a word (by averaging over subword tokens).\n",
    "    \"\"\"\n",
    "    # Tokenize with offsets\n",
    "    tokenizer = model.tokenizer\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, return_tensors='pt')\n",
    "    offsets = encoding['offset_mapping'][0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "    # Get model activations\n",
    "    _, cache = model.run_with_cache([text])\n",
    "    activations = cache[layer_name][0]  # shape: (num_tokens, hidden_dim)\n",
    "    # Map words to token indices\n",
    "    word_spans = []\n",
    "    start = 0\n",
    "    for word in word_list:\n",
    "        end = start + len(word)\n",
    "        word_spans.append((start, end))\n",
    "        start = end + 1  # +1 for space\n",
    "    # For each word, find token indices whose offsets overlap with the word span\n",
    "    word_embeddings = []\n",
    "    for span in word_spans:\n",
    "        indices = [i for i, (s, e) in enumerate(offsets) if not (e <= span[0] or s >= span[1])]\n",
    "        if indices:\n",
    "            emb = activations[indices].mean(dim=0)\n",
    "        else:\n",
    "            emb = torch.zeros(activations.shape[1], device=activations.device)\n",
    "        word_embeddings.append(emb)\n",
    "    return torch.stack(word_embeddings)\n",
    "\n",
    "def distance_matrix_from_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Return (matrix, ids) where matrix[i,j] is the discrete tree distance\n",
    "    (number of edges) between token ids[i] and ids[j].\n",
    "    Expects tokens with integer 'id' and 'head' (head==0 means ROOT).\n",
    "    \"\"\"\n",
    "\n",
    "    forms = [tok['form'] for tok in sentence]\n",
    "    ids = [tok['id'] for tok in sentence if isinstance(tok['id'], int)]\n",
    "    parent = {tok['id']: (tok['head'] if tok['head'] != 0 else None) for tok in sentence if isinstance(tok['id'], int)}\n",
    "    \n",
    "    # Find the root token's id (where head == 0)\n",
    "    root_id = next(tok['id'] for tok in sentence if isinstance(tok['id'], int) and tok['head'] == 0)\n",
    "    id_to_idx = {tid: idx for idx, tid in enumerate(ids)}\n",
    "    root_idx = id_to_idx[root_id]\n",
    "\n",
    "    def ancestors(n):\n",
    "        path = [n]\n",
    "        while parent.get(path[-1]) is not None:\n",
    "            path.append(parent[path[-1]])\n",
    "        return path  # from node up to root\n",
    "\n",
    "    depths = {i: len(ancestors(i)) - 1 for i in ids}  # distance to root\n",
    "    n = len(ids)\n",
    "    mat = np.zeros((n, n), dtype=int)\n",
    "\n",
    "    # compute pairwise distances via lowest common ancestor (LCA)\n",
    "    for i, a in enumerate(ids):\n",
    "        anc_a = ancestors(a)\n",
    "        set_anc_a = set(anc_a)\n",
    "        for j, b in enumerate(ids):\n",
    "            if i == j:\n",
    "                continue\n",
    "            anc_b = ancestors(b)\n",
    "            # find first common ancestor when walking from node b up (closest to b)\n",
    "            lca = next((x for x in anc_b if x in set_anc_a), None)\n",
    "            mat[i, j] = depths[a] + depths[b] - 2 * depths[lca]\n",
    "\n",
    "    root_dist = mat[root_idx, :]\n",
    "    return mat, ids, root_dist, forms\n",
    "\n",
    "def graph_from_distance_matrix(mat, ids, root_dist, forms, ax=None, node_size = 1500, font_size = 12, width = 2, plot = True):\n",
    "\n",
    "    # Compute MST using Kruskal’s algorithm\n",
    "    G = nx.from_numpy_array(mat)\n",
    "    mst = nx.minimum_spanning_tree(G, algorithm=\"kruskal\")\n",
    "    # get adjacency matrix as np array\n",
    "    adj_matrix = nx.to_numpy_array(mst)\n",
    "    # Prepare node positions: x=ids, y=root_dist, to get a readable plot\n",
    "    pos = {i: (id, len(root_dist) - root_dist[idx]) for idx, (i, id) in enumerate(zip(range(len(ids)), ids))}\n",
    "\n",
    "    if plot:\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        nx.draw(\n",
    "            mst,\n",
    "            pos=pos,\n",
    "            ax=ax,\n",
    "            with_labels=True,\n",
    "            labels={i: forms[i] for i in range(len(forms))},\n",
    "            node_color='lightblue',\n",
    "            edge_color='black',\n",
    "            node_size=node_size,\n",
    "            font_size=font_size,\n",
    "            width=width,\n",
    "        )\n",
    "        ax.set_xlabel(\"ids\")\n",
    "        ax.set_ylabel(\"root_dist\")\n",
    "\n",
    "    return adj_matrix, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "601f53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "def eval_parse_trees_by_sentence(test_sentences, predictions, batch_size):\n",
    "    df = pd.DataFrame(columns=['sentence_length', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'sensitivity', 'specificity', 'spearman_rho'])\n",
    "    for sentence_idx, sentence in enumerate(test_sentences):\n",
    "        \n",
    "        batch_id = sentence_idx // batch_size\n",
    "        idx_in_batch = sentence_idx % batch_size\n",
    "\n",
    "        mat, ids, root_dist, forms = distance_matrix_from_sentence(sentence)\n",
    "        n = mat.shape[0]\n",
    "        m1, _ = graph_from_distance_matrix(mat, ids, root_dist, forms, plot = False)\n",
    "\n",
    "        pred_mat = predictions[batch_id][idx_in_batch]\n",
    "        # ensure square and same size\n",
    "        pred_mat = pred_mat[:n, :n]\n",
    "        m2, _ = graph_from_distance_matrix(pred_mat, ids, root_dist, forms, plot = False)\n",
    "\n",
    "        inds = np.triu_indices_from(m1, k=1)\n",
    "        upper_m1 = m1[inds]\n",
    "        upper_m2 = m2[inds]\n",
    "\n",
    "        TP = np.count_nonzero((upper_m1 != 0) & (upper_m2 != 0))\n",
    "        TN = np.count_nonzero((upper_m1 == 0) & (upper_m2 == 0))\n",
    "        FP = np.count_nonzero((upper_m1 == 0) & (upper_m2 != 0))\n",
    "        FN = np.count_nonzero((upper_m1 != 0) & (upper_m2 == 0))\n",
    "    \n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0 # aka recall\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0 \n",
    "\n",
    "        inds = np.triu_indices_from(mat, k=1)\n",
    "        upper_mat = mat[inds]\n",
    "        upper_pred_mat = pred_mat[inds]\n",
    "\n",
    "        rho, _ = spearmanr(upper_mat, upper_pred_mat)\n",
    "\n",
    "        df.loc[sentence_idx] = [n,TP, TN, FP, FN, accuracy, sensitivity, specificity, rho]\n",
    "\n",
    "    return df\n",
    "\n",
    "def eval_parse_trees_by_depth(test_sentences, predictions, batch_size):\n",
    "    total_TP = total_TN = total_FP = total_FN = 0\n",
    "\n",
    "    df = pd.DataFrame(columns=['TP', 'TN', 'FP', 'FN'])\n",
    "\n",
    "    for sentence_idx, sentence in enumerate(test_sentences):\n",
    "        batch_id = sentence_idx // batch_size\n",
    "        idx_in_batch = sentence_idx % batch_size\n",
    "\n",
    "        mat, ids, root_dist, forms = distance_matrix_from_sentence(sentence)\n",
    "        n = mat.shape[0]\n",
    "        m1, _ = graph_from_distance_matrix(mat, ids, root_dist, forms, plot = False)\n",
    "\n",
    "        pred_mat = predictions[batch_id][idx_in_batch]\n",
    "        pred_mat = pred_mat[:n, :n]\n",
    "\n",
    "        m2, _ = graph_from_distance_matrix(pred_mat, ids, root_dist, forms, plot = False)\n",
    "\n",
    "        inds = np.triu_indices(n, k=1)\n",
    "        for i, j in zip(inds[0], inds[1]):\n",
    "            lin_dist = abs(i - j) if abs(i - j) < 10 else 10 # bin all values >= 10 together\n",
    "            if lin_dist not in df.index:\n",
    "                df.loc[lin_dist] = [0, 0, 0, 0]\n",
    "\n",
    "            gold_edge = (m1[i, j] != 0)\n",
    "            pred_edge = (m2[i, j] != 0)\n",
    "\n",
    "            if gold_edge and pred_edge:\n",
    "                total_TP += 1\n",
    "                df.at[lin_dist, 'TP'] += 1\n",
    "            elif (not gold_edge) and (not pred_edge):\n",
    "                total_TN += 1\n",
    "                df.at[lin_dist, 'TN'] += 1\n",
    "            elif (not gold_edge) and pred_edge:\n",
    "                total_FP += 1\n",
    "                df.at[lin_dist, 'FP'] += 1\n",
    "            elif gold_edge and (not pred_edge):\n",
    "                total_FN += 1\n",
    "                df.at[lin_dist, 'FN'] += 1\n",
    "\n",
    "    # safe per-distance metrics\n",
    "    df['accuracy'] = (df['TP'] + df['TN']) / (df['TP'] + df['TN'] + df['FP'] + df['FN'])\n",
    "    df['sensitivity'] = np.where((df['TP'] + df['FN']) > 0, df['TP'] / (df['TP'] + df['FN']), 0)\n",
    "    df['specificity'] = np.where((df['TN'] + df['FP']) > 0, df['TN'] / (df['TN'] + df['FP']), 0)\n",
    "    df = df.fillna(0).sort_index()\n",
    "    return df\n",
    "\n",
    "class IdentityModel(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "model = IdentityModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67bc1363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing TwoWordPSDProbe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TwoWordPSDProbe()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up args as in training\n",
    "layer = 'blocks.8.hook_resid_post'\n",
    "args = {\n",
    "    'probe': {'maximum_rank': 64, 'params_path': f'probe_params/{layer}_probe_params.pt'},\n",
    "    'model': {'hidden_dim': 768},\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'probe_training': {'epochs': 20},\n",
    "    'reporting': {'root': './'}\n",
    "}\n",
    "# Load probe and parameters\n",
    "probe_test = TwoWordPSDProbe(args)\n",
    "state_dict = torch.load(args['probe']['params_path'], map_location=args['device'])\n",
    "probe_test.load_state_dict(state_dict)\n",
    "probe_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5634d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sentences_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3ebb035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: hook_embed\n",
      "Constructing TwoWordPSDProbe\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 2004/2004 [00:56<00:00, 35.42it/s]\n",
      "[predicting]: 100%|██████████| 21/21 [00:00<00:00, 50.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: blocks.0.hook_resid_pre\n",
      "Constructing TwoWordPSDProbe\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 2004/2004 [00:55<00:00, 36.14it/s]\n",
      "[predicting]: 100%|██████████| 21/21 [00:00<00:00, 58.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: blocks.0.hook_resid_post\n",
      "Constructing TwoWordPSDProbe\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 2004/2004 [00:53<00:00, 37.62it/s]\n",
      "[predicting]: 100%|██████████| 21/21 [00:00<00:00, 35.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: blocks.2.hook_resid_post\n",
      "Constructing TwoWordPSDProbe\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 2004/2004 [00:53<00:00, 37.31it/s]\n",
      "[predicting]: 100%|██████████| 21/21 [00:00<00:00, 35.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: blocks.4.hook_resid_post\n",
      "Constructing TwoWordPSDProbe\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 2004/2004 [00:55<00:00, 36.41it/s]\n",
      "[predicting]: 100%|██████████| 21/21 [00:00<00:00, 58.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: blocks.6.hook_resid_post\n",
      "Constructing TwoWordPSDProbe\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 2004/2004 [00:54<00:00, 36.56it/s]\n",
      "[predicting]: 100%|██████████| 21/21 [00:00<00:00, 45.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: blocks.8.hook_resid_post\n",
      "Constructing TwoWordPSDProbe\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 2004/2004 [00:55<00:00, 36.27it/s]\n",
      "[predicting]: 100%|██████████| 21/21 [00:00<00:00, 59.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: blocks.10.hook_resid_post\n",
      "No probe parameters found for layer blocks.10.hook_resid_post, skipping.\n",
      "Processing layer: blocks.11.hook_resid_post\n",
      "No probe parameters found for layer blocks.11.hook_resid_post, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "probes = {}\n",
    "layer_names = [\n",
    "    'hook_embed',\n",
    "    'blocks.0.hook_resid_pre',\n",
    "    'blocks.0.hook_resid_post', \n",
    "    'blocks.2.hook_resid_post', \n",
    "    'blocks.4.hook_resid_post',\n",
    "    'blocks.6.hook_resid_post',\n",
    "    'blocks.8.hook_resid_post',\n",
    "    'blocks.10.hook_resid_post',\n",
    "    'blocks.11.hook_resid_post'\n",
    "]\n",
    "for layer_name in layer_names:\n",
    "    print(f'Processing layer: {layer_name}')\n",
    "\n",
    "    if f'{layer_name}_probe_params.pt' not in os.listdir('probe_params'):\n",
    "        print(f'No probe parameters found for layer {layer_name}, skipping.')\n",
    "        continue\n",
    "    args = {\n",
    "        'probe': {'maximum_rank': 64, 'params_path': f'probe_params/{layer_name}_probe_params.pt'},\n",
    "        'model': {'hidden_dim': 768},\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'probe_training': {'epochs': 20},\n",
    "        'reporting': {'root': './'}\n",
    "    }\n",
    "    # Load probe and parameters\n",
    "    probe_test = TwoWordPSDProbe(args)\n",
    "    state_dict = torch.load(args['probe']['params_path'], map_location=args['device'])\n",
    "    probe_test.load_state_dict(state_dict)\n",
    "    probe_test.eval()\n",
    "\n",
    "    # Use IdentityModel as in your code\n",
    "    class IdentityModel(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x\n",
    "    model = IdentityModel()\n",
    "        \n",
    "    novel_dataset = SyntaxProbeDataset(sentences_test, 'gpt2-small', layer_name)\n",
    "    novel_dataloader = DataLoader(novel_dataset, batch_size=100, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    regimen = ProbeRegimen(args)\n",
    "\n",
    "    predictions = regimen.predict(probe_test, model=IdentityModel(), dataset=novel_dataloader)\n",
    "\n",
    "    probes[layer_name] = predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos = [eval_parse_trees_by_sentence(test_sentences=sentences_test, predictions=probes[layer_name], batch_size=100).mean().loc['spearman_rho'] for layer_name in layer_names if layer_name in probes.keys()]\n",
    "rhos\n",
    "plt.plot(probes.keys(), rhos, marker='o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntax_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
